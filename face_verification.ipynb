{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Computer vision project\n",
        "## Mahsa Bahri - 98243011"
      ],
      "metadata": {
        "id": "mOV00RmfZJVZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2-e8miS_ioN",
        "outputId": "4120cf0d-8537-478a-a9b7-55a42a6c0f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.insert(0,'/content/drive/My Drive/ComputerVisionProject/FaceRecognition')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Face verification:\n",
        "\n",
        "**Face recognition** is the technology that allows computers and machines to match images containing people's faces and their identities.\n",
        "\n",
        "<div align=\"center\"><img src=https://drive.google.com/uc?export=view&id=1xl32yoGng2q6YZq0yRr4n3hjQjjmd1Vo >\n",
        "</div>\n",
        "\n",
        "**Face verification**: For example, at some airports, you can pass through customs by letting a system scan your passport and then verifying that you (the person carrying the passport) are the correct person. A mobile phone that unlocks using your face is also using face verification. This is a 1:1 matching problem.\n",
        "\n",
        "\n",
        "Face recognition can be divided into multiple steps. The image below shows an example of a face recognition pipeline.\n",
        "\n",
        "1.   Face detection — Detecting one or more faces in an image.\n",
        "2.   Feature extraction — Extracting the most important features from an image of the face.\n",
        "3. Face classification — Classifying the face based on extracted features.\n",
        "\n"
      ],
      "metadata": {
        "id": "MIXCxbq8Hyjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FaceNet:\n",
        "We are going to build a face recognition system. Our idea is based on FaceNet model. but what is FaceNet. but, what is faceNet exactly?\n",
        "\n",
        "**FaceNet** is a deep neural network used for extracting features from an image of a person’s face.\n",
        "\n",
        "How does FaceNet work?\n",
        "\n",
        "<div align=\"center\"><img src=https://arsfutura-production.s3.us-east-1.amazonaws.com/magazine/2019/10/face_recognition/facenet-brki.png width = 500></div>\n",
        "\n",
        "FaceNet takes an image of the person’s face as input and outputs a vector of 128 numbers which represent the most important features of a face. In machine learning, this vector is called embedding.\n",
        "\n",
        "Ok, what do we do with these embeddings? How do we recognise a person using an embedding?\n",
        "\n",
        "Embeddings are vectors and we can interpret vectors as points in the Cartesian coordinate system. That means we can plot an image of a face in the coordinate system using its embeddings.\n",
        "\n",
        "<div align=\"center\"><img src= https://arsfutura-production.s3.us-east-1.amazonaws.com/magazine/2019/10/face_recognition/facenet-brki-ana.png width = 500></div>\n",
        "\n",
        "One possible way of recognising a person on an unseen image would be to calculate its embedding, calculate distances to images of known people and if the face embedding is close enough to embeddings of person A, we say that this image contains the face of person A.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s2E5F0QaNO72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Channels-first notation\n",
        "\n",
        "* In this exercise, we will be using a pre-trained model which represents ConvNet activations using a **\"channels first\"** convention.\n",
        "\n",
        "## 1 - Encoding face images into a 128-dimensional vector\n",
        "\n",
        "### 1.1 - Using a ConvNet  to compute encodings\n",
        "\n",
        "The FaceNet model takes a lot of data and a long time to train. So following common practice in applied deep learning, let's  load weights that someone else has already trained.\n",
        "<br></br>\n",
        "\n",
        "Let's start with importing packages:\n"
      ],
      "metadata": {
        "id": "LlLuF0R8Roy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from tensorflow.keras.layers import BatchNormalization, Layer\n",
        "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from fr_utils import *\n",
        "from inception_blocks_v2 import *\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)"
      ],
      "metadata": {
        "id": "3YdkTGtlAken",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c830d6-6d12-4ed7-8f1f-e34e22f8ce22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key things you need to know are:\n",
        "\n",
        "- This network uses 96x96 dimensional RGB images as its input. Specifically, inputs a face image (or batch of $m$ face images) as a tensor of shape $(m, n_C, n_H, n_W) = (m, 3, 96, 96)$\n",
        "- It outputs a matrix of shape $(m, 128)$ that encodes each input face image into a 128-dimensional vector\n",
        "\n",
        "Run the cell below to create the model for face images."
      ],
      "metadata": {
        "id": "BBjx3KfpTyoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n",
        "print(\"Total Params:\", FRmodel.count_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVPjeKF4ArBd",
        "outputId": "a6115c60-aa90-4357-91a3-fea2d671d99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Params: 3743280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using a 128-neuron fully connected layer as its last layer, the model ensures that the output is an encoding vector of size 128. You then use the encodings to compare two face images as follows:\n",
        "\n",
        "\n",
        "<img src= \"https://drive.google.com/uc?export=view&id=1g1SiAAOZRVYTgVq0EfDl7rO2ZsI-NGP0\" style=\"width:380px;height:150px;\">\n",
        "\n",
        "By computing the distance between two encodings and thresholding, you can determine if the two pictures represent the same person\n",
        "\n",
        "So, an encoding is a good one if:\n",
        "- The encodings of two images of the same person are quite similar to each other.\n",
        "- The encodings of two images of different persons are very different.\n",
        "\n",
        "The triplet loss function formalizes this, and tries to \"push\" the encodings of two images of the same person (Anchor and Positive) closer together, while \"pulling\" the encodings of two images of different persons (Anchor, Negative) further apart.\n",
        "\n",
        "### 1.2 - The Triplet Loss\n",
        "\n",
        "For an image $x$, we denote its encoding $f(x)$, where $f$ is the function computed by the neural network.\n",
        "\n",
        "<img src= \"https://drive.google.com/uc?export=view&id=1JJBID2gNhK08omJpURxd_KWncm1tK3dF\" style=\"width:380px;height:150px;\">\n",
        "\n",
        "\n",
        "<!--\n",
        "We will also add a normalization step at the end of our model so that $\\mid \\mid f(x) \\mid \\mid_2 = 1$ (means the vector of encoding should be of norm 1).\n",
        "!-->\n",
        "\n",
        "Training will use triplets of images $(A, P, N)$:  \n",
        "\n",
        "- A is an \"Anchor\" image--a picture of a person.\n",
        "- P is a \"Positive\" image--a picture of the same person as the Anchor image.\n",
        "- N is a \"Negative\" image--a picture of a different person than the Anchor image.\n",
        "\n",
        "These triplets are picked from our training dataset. We will write $(A^{(i)}, P^{(i)}, N^{(i)})$ to denote the $i$-th training example.\n",
        "\n",
        "You'd like to make sure that an image $A^{(i)}$ of an individual is closer to the Positive $P^{(i)}$ than to the Negative image $N^{(i)}$) by at least a margin $\\alpha$:\n",
        "\n",
        "$$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 + \\alpha < \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$$\n",
        "\n",
        "You would thus like to minimize the following \"triplet cost\":\n",
        "\n",
        "$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\underbrace{\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha \\large ] \\small_+ \\tag{3}$$\n",
        "\n",
        "Here, we are using the notation \"$[z]_+$\" to denote $max(z,0)$.  \n",
        "\n",
        "Notes:\n",
        "- The term (1) is the squared distance between the anchor \"A\" and the positive \"P\" for a given triplet; you want this to be small.\n",
        "- The term (2) is the squared distance between the anchor \"A\" and the negative \"N\" for a given triplet, you want this to be relatively large. It has a minus sign preceding it because minimizing the negative of the term is the same as maximizing that term.\n",
        "- $\\alpha$ is called the margin. It is a hyperparameter that you pick manually. We will use $\\alpha = 0.2$.\n",
        "\n",
        "Most implementations also rescale the encoding vectors to haven L2 norm equal to one (i.e., $\\mid \\mid f(img)\\mid \\mid_2$=1); you won't have to worry about that in this assignment.\n",
        "\n",
        "**Exercise**: Implement the triplet loss as defined by formula (3). Here are the 4 steps:\n",
        "1. Compute the distance between the encodings of \"anchor\" and \"positive\": $\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2$\n",
        "2. Compute the distance between the encodings of \"anchor\" and \"negative\": $\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$\n",
        "3. Compute the formula per training example: $ \\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 - \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2 + \\alpha$\n",
        "3. Compute the full formula by taking the max with zero and summing over the training examples:\n",
        "$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 - \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2+ \\alpha \\large ] \\small_+ \\tag{3}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "Vt5zv8G5U0ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
        "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
        "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis = -1)\n",
        "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis = -1)\n",
        "    basic_loss = pos_dist- neg_dist + alpha\n",
        "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "1BWk15_kAyBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Loading the pre-trained model\n",
        "\n",
        "FaceNet is trained by minimizing the triplet loss. But since training requires a lot of data and a lot of computation, we won't train it from scratch here. Instead, we load a previously trained model. Load a model using the following cell; this might take a couple of minutes to run."
      ],
      "metadata": {
        "id": "Et_TmexJYmZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
        "load_weights_from_FaceNet(FRmodel)"
      ],
      "metadata": {
        "id": "bepkCSQEA6ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a database containing one encoding vector for each person who is allowed to enter the office. To generate the encoding we use `img_to_encoding(image_path, model)`, which runs the forward propagation of the model on the specified image.\n",
        "\n",
        "Run the following code to build the database (represented as a python dictionary). This database maps each person's name to a 128-dimensional encoding of their face."
      ],
      "metadata": {
        "id": "8Nb_fkvbZV5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def img_to_encoding(image_path, model):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path)\n",
        "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
        "    x_train = np.expand_dims(img, axis=0)\n",
        "    embedding = model.predict_on_batch(x_train)\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "joiy7M_KTFpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a database containing one encoding vector for each person"
      ],
      "metadata": {
        "id": "_fOkaBcKc8ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "database = {}\n",
        "base_image_path = \"/content/drive/My Drive/ComputerVisionProject/FaceRecognition/images/\"\n",
        "\n",
        "database[\"Messi1\"] = img_to_encoding(base_image_path+\"Messi1.jpeg\", FRmodel)\n",
        "database[\"Messi2\"] = img_to_encoding(base_image_path+\"Messi2.jpeg\", FRmodel)\n",
        "database[\"Brad Pitt1\"] = img_to_encoding(base_image_path+\"Brad_Pitt1.jpeg\", FRmodel)\n",
        "database[\"Brad Pitt2\"] = img_to_encoding(base_image_path+\"Brad_Pitt2.jpeg\", FRmodel)\n",
        "database[\"Dicaprio1\"] = img_to_encoding(base_image_path+\"dicaprio1.jpeg\", FRmodel)\n",
        "database[\"Dicaprio2\"] = img_to_encoding(base_image_path+\"dicaprio2.jpg\", FRmodel)\n",
        "database[\"Tom Hardy1\"] = img_to_encoding(base_image_path+\"Tom_Hardy1.jpg\", FRmodel)\n",
        "database[\"Tom Hardy2\"] = img_to_encoding(base_image_path+\"Tom_Hardy2.jpg\", FRmodel)\n"
      ],
      "metadata": {
        "id": "E0AJbRZ_aHrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Verify\n",
        "\n",
        "Implementation of verify() function:\n",
        "\n",
        "\n",
        "1.  Compute the encoding of the image from `image_path`.\n",
        "2. Compute the distance between this encoding and the encoding of the identity image stored in the database.\n",
        "3. The result will be \"Same person\", if the distance is less than 0.7, else \"Different persons\".\n",
        "\n"
      ],
      "metadata": {
        "id": "sSXrGXXScq00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_by_name(image_path, identity, database, model):\n",
        "    encoding = img_to_encoding(image_path, model)\n",
        "    dist = np.linalg.norm(encoding - database[identity])\n",
        "    if dist < 0.8:\n",
        "        print(\"Same person\")\n",
        "    else:\n",
        "        print(\"Different persons\")\n",
        "    return dist\n",
        "\n",
        "def verify_by_iamge(src_image_path, dest_image_path, database, model):\n",
        "    src_encoding = img_to_encoding(src_image_path, model)\n",
        "    dest_encoding = img_to_encoding(dest_image_path, model)\n",
        "    dist = np.linalg.norm(src_encoding - dest_encoding)\n",
        "    if dist < 0.8:\n",
        "        print(\"Same person\")\n",
        "    else:\n",
        "        print(\"Different persons\")\n",
        "    return dist"
      ],
      "metadata": {
        "id": "R1kcETildo7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Vhjmgzobmysgu3xe-_aKFCOfVqelo_7v\" >\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=12NlFynmoW0posFko96Dhe4y6EX8ligmQ\">"
      ],
      "metadata": {
        "id": "H8L4DRi_QAfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verify_by_iamge(base_image_path+\"Messi1.jpeg\", base_image_path+\"Messi2.jpeg\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spb_Fyvvfbj2",
        "outputId": "86eacb2e-13b7-4865-9d31-af1c3c3e69b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 96, 96)\n",
            "(1, 3, 96, 96)\n",
            "Same person\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7961557"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1GsfY09FrIoslko4NU8Ipl-n_S3_1E9-b\" >\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=17EQ_2fY_wunhu1QI5yDdhFfr3dJhfO7p\">\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fa9-LUxcO2tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verify_by_iamge(base_image_path+\"Tom_Hardy1.jpg\", base_image_path+\"Tom_Hardy2.jpg\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVSM0aJqL5Rv",
        "outputId": "539ad85a-ece3-4aff-c22f-fb4a3abe465a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 96, 96)\n",
            "(1, 3, 96, 96)\n",
            "Same person\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7357289"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1GsfY09FrIoslko4NU8Ipl-n_S3_1E9-b\" >\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Vhjmgzobmysgu3xe-_aKFCOfVqelo_7v\" >\n"
      ],
      "metadata": {
        "id": "bVQt96WOQLRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verify_by_iamge(base_image_path+\"Tom_Hardy1.jpg\", base_image_path+\"Messi1.jpeg\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMalmC97MoTr",
        "outputId": "e86aadbe-121d-47df-ff90-b7b7c112753a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 96, 96)\n",
            "(1, 3, 96, 96)\n",
            "Different persons\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8464964"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H_SuC0pDQ60YX1SHsc_UuHqDeS_5AxUN\" >\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1LnVLtRsqULwK-MmiNN5NIO4MjtzTbQI0\" >\n"
      ],
      "metadata": {
        "id": "iw0vSYOFQUA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verify_by_iamge(base_image_path+\"dicaprio1.jpeg\", base_image_path+\"dicaprio2.jpg\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU1qPpOUzsAC",
        "outputId": "c9b79451-5baa-4bf1-eaaa-a99bccd809ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 96, 96)\n",
            "(1, 3, 96, 96)\n",
            "Same person\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.52137023"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H_SuC0pDQ60YX1SHsc_UuHqDeS_5AxUN\" >\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1GsfY09FrIoslko4NU8Ipl-n_S3_1E9-b\" >"
      ],
      "metadata": {
        "id": "y6k5D2lCRDCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verify_by_iamge(base_image_path+\"dicaprio1.jpeg\", base_image_path+\"Tom_Hardy1.jpg\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc3bZ6QKMerO",
        "outputId": "df0eafef-7e23-499f-a4d6-a89a45f30c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Different persons\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.88185465"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=18NCBEe7sdlpRyGDHAU9nZdxHirMmlnTe\" >\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-IPNEIAAwJK5P4qXcD840sU1pizI9HcJ\" >"
      ],
      "metadata": {
        "id": "dWUtr-FERM1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verify_by_iamge(base_image_path+\"Brad_Pitt1.jpeg\", base_image_path+\"Brad_Pitt2.jpeg\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgddylX9140W",
        "outputId": "b347830e-694d-49f7-e2b4-acd34881f288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Same person\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6201238"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1Vhjmgzobmysgu3xe-_aKFCOfVqelo_7v\" >\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1H_SuC0pDQ60YX1SHsc_UuHqDeS_5AxUN\" >"
      ],
      "metadata": {
        "id": "RO1VBGIQRc0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verify_by_iamge(base_image_path+\"Messi1.jpeg\", base_image_path+\"dicaprio1.jpeg\", database, FRmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odm-eXfy4Mim",
        "outputId": "367e53f5-1c10-403f-c653-35ee8ef9ad09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Different persons\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8456635"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative Approaches to Similarity Learning\n",
        "\n",
        "### Ranking Loss Functions: Metric Learning\n",
        "Unlike other loss functions, such as Cross-Entropy Loss or Mean Square Error Loss, whose objective is to learn to predict directly a label, a value, or a set or values given an input, the objective of Ranking Losses is to predict relative distances between inputs. This task if often called metric learning.\n",
        "\n",
        "Ranking Losses functions are very flexible in terms of training data: We just need a similarity score between data points to use them. That score can be binary (similar / dissimilar). As an example, imagine a face verification dataset, where we know which face images belong to the same person (similar), and which not (dissimilar). Using a Ranking Loss function, we can train a CNN to infer if two face images belong to the same person or not.\n",
        "To use a Ranking Loss function we first extract features from two (or three) input data points and get an embedded representation for each of them. Then, we define a metric function to measure the similarity between those representations, for instance euclidian distance. Finally, we train the feature extractors to produce similar representations for both inputs, in case the inputs are similar, or distant representations for the two inputs, in case they are dissimilar.\n",
        "We don’t even care about the values of the representations, only about the distances between them. However, this training methodology has demonstrated to produce powerful representations for different tasks.\n",
        "\n",
        "###Pairwise Ranking Loss\n",
        "\n",
        "\n",
        "###ArcFace Loss and the Angle Margin Penalty\n",
        "There are many alternatives to the triplet loss, one of them is the ArcFace Loss. This is a loss based on the cross-entropy loss aiming to maximize the decision boundary between classes thus grouping similar data points closer together.\n",
        "The idea behind ArcFace is that it maximizes the angle between interclass and minimizes the angle between intraclass on a hypersphere. We then add the angular margin penalty which is inserted between the weight of the true logit and the embedding. This adds a angle penalty to the original angle between the logit and the embedding.\n",
        "\n",
        "The angle margin penalty helps in penalizing the embedding vectors that goes far and help in bringing the embedding features of a certain class come more closer."
      ],
      "metadata": {
        "id": "G9dcEi1OUiRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are Vector Embeddings?\n",
        "ML algorithms, like most software algorithms, need numbers to work with. Sometimes we have a dataset with columns of numeric values or values that can be translated into them (ordinal, categorical, etc). Other times we come across something more abstract like an entire document of text. We create vector embeddings, which are just lists of numbers, for data like this to perform various operations with them. A whole paragraph of text or any other object can be reduced to a vector. Even numerical data can be turned into vectors for easier operations.\n",
        "\n",
        "<img src= \"https://d33wubrfki0l68.cloudfront.net/4b4ae6760dab99a18438671111f77e28498a2fb4/1093c/images/vector_embeddings.jpg\" style=\"width:380px;height:150px;\">\n",
        "\n",
        "###Creating Vector Embeddings\n",
        "One way of creating vector embeddings is to engineer the vector values using domain knowledge. This is known as feature engineering. For example, in medical imaging, we use medical expertise to quantify a set of features such as shape, color, and regions in an image that capture the semantics. However, engineering vector embeddings requires domain knowledge, and it is too expensive to scale.\n",
        "\n",
        "Instead of engineering vector embeddings, we often train models to translate objects to vectors. A deep neural network is a common tool for training such models. The resulting embeddings are usually high dimensional (up to two thousand dimensions) and dense (all values are non-zero). For text data, models such as Word2Vec, GLoVE, and BERT transform words, sentences, or paragraphs into vector embeddings.\n"
      ],
      "metadata": {
        "id": "-gVtA4smYPAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References:\n",
        "\n",
        "- The explenation of FaceNet took from this [website](https://arsfutura.com/magazine/face-recognition-with-facenet-and-mtcnn/).\n",
        "- I got more information about similarity learning from these two websites: [website1](https://gombru.github.io/2019/04/03/ranking_loss/) ,  [website2](https://towardsdatascience.com/novel-approaches-to-similarity-learning-e680c61d53cd#:~:text=Alternatives%3A%20ArcFace%20Loss%20and%20the,similar%20data%20points%20closer%20together.).\n",
        "-You can more about vector embedding, [here](https://www.pinecone.io/learn/vector-embeddings/).\n",
        "- The pretrained model we use is inspired by Victor Sy Wang's implementation and was loaded using his [code](https://github.com/iwantooxxoox/Keras-OpenFace).\n",
        "- Our implementation also took a lot of inspiration from this [repository](https://github.com/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Face%20Recognition/Face_Recognition_v3a.ipynb)\n"
      ],
      "metadata": {
        "id": "8sAAgiy5iQJd"
      }
    }
  ]
}